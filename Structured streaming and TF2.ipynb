{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Structured Streaming and Tensorflow 2 integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType, BinaryType\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from typing import Iterator, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f94046ef0d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start a Spark session manually\n",
    "findspark.init()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"./model\"\n",
    "img_dir = \"./img\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a small Tensorflow 2 model on the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data has shape (60000, 28, 28)\n",
      "The test data has shape (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Get some test data for trainng\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(\"The train data has shape {}\".format(x_train.shape))\n",
    "print(\"The test data has shape {}\".format(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a small Tensorflow model\n",
    "def get_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2994 - accuracy: 0.9126\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1446 - accuracy: 0.9570\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1083 - accuracy: 0.9676\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0885 - accuracy: 0.9725\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0754 - accuracy: 0.9767\n",
      "WARNING:tensorflow:From /home/sanne/Envs/pyspark/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/sanne/Envs/pyspark/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    }
   ],
   "source": [
    "# Train and save the model\n",
    "model = get_model()\n",
    "model.summary()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam',loss=loss_fn, metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the model and report some metrices\n",
    "new_model = load_model(model_file)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test data is 0.9778\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(new_model.predict(x_test), axis=1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"The accuracy on the test data is {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Spark structured stream deom file using the TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType, BinaryType\n",
    "from typing import Iterator, Tuple\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract label from path\n",
    "@pandas_udf(\"string\")\n",
    "def extract_label(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for paths in iterator:\n",
    "        labels = [path[-5] for idx, path in paths.iteritems()]\n",
    "        yield pd.Series(labels)\n",
    "\n",
    "# Get the image data from the bytebuffer as read by Spark\n",
    "def decode_image(data):\n",
    "    return np.array(Image.open(io.BytesIO(data)))\n",
    "\n",
    "# Predict the label of an image given a model\n",
    "def predict_label(model, image_data):\n",
    "    data = decode_image(image_data)\n",
    "    data = np.expand_dims(data, 0)/255.0\n",
    "    prediction = np.argmax(model(data).numpy())\n",
    "    return str(prediction)\n",
    "\n",
    "# Embed the TF model in an UDF\n",
    "@pandas_udf(\"string\")\n",
    "def image_rec(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    model = load_model(model_file)\n",
    "    for content in iterator:\n",
    "        predictions = [predict_label(model, image_data) for idx, image_data in content.iteritems()]\n",
    "        yield pd.Series(predictions)\n",
    "    \n",
    "# Define the input stream\n",
    "input_schema = StructType([\n",
    "  StructField(\"path\", StringType(), False),\n",
    "  StructField(\"modificationTime\", TimestampType(), False),\n",
    "  StructField(\"length\", LongType(), False),\n",
    "  StructField(\"content\", BinaryType(), True)\n",
    "])\n",
    "    \n",
    "img_stream = (spark\n",
    "              .readStream\n",
    "              .schema(input_schema)\n",
    "              .format(\"binaryFile\")\n",
    "              .load(\"./img\"))\n",
    "\n",
    "# Define the query with the TF model\n",
    "query = (img_stream\n",
    "    .select(extract_label(\"path\"), image_rec(\"content\"))\n",
    "    .withColumnRenamed('extract_label(path)', 'label')\n",
    "    .withColumnRenamed('image_rec(content)', 'prediction'))\n",
    "\n",
    "# Define the output location\n",
    "output = (query\n",
    "          .writeStream\n",
    "          .outputMode(\"append\")\n",
    "          .option(\"checkpointLocation\", \"./checkpoint\")\n",
    "          .format(\"console\")\n",
    "          .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f945405a710>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Export some test images to the drop dir and show an example image\n",
    "os.makedirs(img_dir, exist_ok = True) \n",
    "for i in range(10):\n",
    "    image_data =  (255 * np.squeeze(x_test[1000 + i])).astype(np.uint8)\n",
    "    label = y_test[1000 + i]\n",
    "    img = Image.fromarray(image_data, 'L') # L is the greyscale mode\n",
    "    img.save(\"{}/img{}-lbl{}.jpg\".format(img_dir, i, label))\n",
    "    \n",
    "imshow(image_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```-------------------------------------------\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+-----+----------+\n",
    "|label|prediction|\n",
    "+-----+----------+\n",
    "|    5|         5|\n",
    "|    0|         0|\n",
    "|    2|         2|\n",
    "|    7|         7|\n",
    "|    9|         9|\n",
    "|    0|         0|\n",
    "|    1|         1|\n",
    "+-----+----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop any active stream with this command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stream in spark.streams.active:\n",
    "    if stream.isActive: stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIN\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
